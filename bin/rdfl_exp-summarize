#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import datetime
import json
import os
import shutil
from os import listdir
from os.path import isfile, join

import pandas as pd
import paramiko
from matplotlib import pyplot as plt

from rdfl_exp import config
from rdfl_exp.utils import csv
from rdfl_exp.utils.terminal import progress_bar

host = "10.240.5.104"
port = 22
username = "ubuntu"
password = "ubuntu"


# exp_to_fetch = "20210813_132154"  # faf + dk
# exp_to_fetch = "20210817_095456"  # faf
# exp_to_fetch = "20210817_211215"  # baf

# exp_to_fetch = "20210818_225415"  # Unknown Reason + faf
# exp_to_fetch = "20210819_143747"
# exp_to_fetch = "20210819_173227"

exp_to_fetch = "20210914_211805"

report_dir      = os.path.join(config.RDFL_ROOT, "report")
res_dir         = os.path.join(report_dir, "rdfl_exp_summary_{}".format(exp_to_fetch))
datasets_dir   = join(res_dir, "datasets")
remote_out_dir  = "/home/ubuntu/.rdfl_exp/out"


# ===== ( Helper functions ) ===========================================================================================

def _create_folder_structure():

    # Create root and report folder
    dirs = [config.RDFL_ROOT, report_dir]

    for d in dirs:
        try:
            os.mkdir(d)
        except OSError:
            pass

    if os.path.exists(res_dir):
        shutil.rmtree(res_dir)
    os.mkdir(res_dir)
    os.mkdir(datasets_dir)
# End def create_folder_structure


def _fetch_output_files():
    # Create the ssh connection
    print("Establishing SSH connection to {}@{}:{}".format(username, host, port))
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(host, port, username, password)

    # List all the experiments that can be listed
    stdin, stdout, stderr = ssh.exec_command("ls {}".format(remote_out_dir))
    experiments = sorted([x.strip() for x in stdout.readlines()])

    # Get the experiment we want to fetch
    last_exp = experiments[experiments.index(exp_to_fetch)]

    # Get the file from the experiment we want to fetch
    transport = paramiko.Transport((host, port))
    sftp = None
    try:
        print("Establishing the transport pipe")
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)

        print("Getting the statistics file")
        sftp.get(join(remote_out_dir, last_exp, "stats.json"), join(res_dir, "stats.json"))
        # Get the datasets

        print("Getting the datasets")
        file_list = sftp.listdir(join(remote_out_dir, last_exp, "datasets"))
        it = 1
        progress_bar(0, len(file_list),
                     prefix='Progress:',
                     suffix='Complete ({}/{})'.format(0, len(file_list)),
                     length=100)
        for item in file_list:
            sftp.get(join(remote_out_dir, last_exp, "datasets", item), join(datasets_dir, item))
            progress_bar(it, len(file_list),
                         prefix='Progress:',
                         suffix='Complete ({}/{})'.format(it, len(file_list)),
                         length=100)
            it += 1

    finally:
        if transport:
            transport.close()
        if sftp:
            sftp.close()
# End def fetch_output_files


def _generate_report(stats: dict):

    #
    rules = stats["classifier"]["rules"]
    target_class = stats['context']['target_class']

    # Write about the context
    lines = [
        "======== Context ========\n\n",
        "Precision Threshold: {:.2f}%\n".format(stats['context']['precision_threshold'] * 100.0),
        "Recall Threshold: {:.2f}%\n".format(stats['context']['recall_threshold'] * 100.0),
        "target_class: {}\n".format(stats['context']['target_class']),
        "Samples generated per iterations: {}\n".format(stats['context']['samples_per_iteration']),
        "Data formatting method used: {}\n\n".format(stats['context']['data_format_method']),
        "======== Iteration Summary ========\n\n"
    ]
    for i in range(stats["context"]["iterations"]):
        # First compute the rule string
        rule_str = ""
        nb_of_rules = 0
        for r in rules[i]:
            nb_of_rules += 1
            rule_str += "\t\t{}\n".format(r)

        # Iteration header
        lines.append("=> iteration :{}\n\n".format(i))

        # Write the precision and recall stats achieved
        lines.append("\tPrecision for target class: {}\n".format(stats["classifier"][target_class]["precision"][i]))
        lines.append("\tRecall for target class: {}\n\n".format(stats["classifier"][target_class]["recall"][i]))

        # Write the time used per iteration:
        iteration_time = str(datetime.timedelta(seconds=float(stats["timing"]["iteration"][i])))
        learning_time = str(datetime.timedelta(seconds=float(stats["timing"]["learning"][i])))
        lines.append("\tTime taken to finish the iteration: {}\n".format(iteration_time))
        lines.append("\ttime taken to generate the classifier: {}\n\n".format(learning_time))

        # Add the rules
        lines.append("\tNumber of Rules generated: {}\n".format(nb_of_rules))
        if nb_of_rules > 0:
            lines.append("\tRules:\n")
            lines.append(rule_str)
        lines.append("\n")

    # Write the lines to the report file
    with open(os.path.join(res_dir, "report.txt"), 'w') as f:
        f.writelines(lines)

# End def generate_report


def _generate_graphics(stats: dict, show=False):
    """Generate the graphics for the report
    :param stats: The statistics dictionnary
    :param show: If set to True, the graphs are display using plt
    """

    it_ax = list(range(stats["context"]["iterations"]))
    print(stats)
    target_class = stats["context"]["target_class"]
    other_class  = stats["context"]["other_class"]

    # update the font size
    plt.rcParams.update({'font.size': 22})

    # 1st Graph: Accuracy, TP/FP rate for target_class, TP/FP rate for other class
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the precision graph
    axs[0].plot(it_ax,
                stats['classifier']["accuracy"],
                ls='-',
                marker='x',
                color='blue',
                label="accuracy_score")
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('Classifier Accuracy per iteration')

    ### Draw the recall graph
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('TP/FP Rate for {} per iteration'.format(target_class))

    ### Draw the recall graph
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('TP/FP Rate for {} per iteration'.format(other_class))

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'classifier_accuracy_graph.png'))

    # 2nd Graph: precision, recall, PRC graph
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the precision graph
    axs[0].axhline(y=stats["context"]["precision_threshold"], color='r', linestyle='--')
    axs[0].plot(it_ax,
                stats['classifier'][target_class]["precision"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[0].plot(it_ax,
                stats['classifier'][other_class]["precision"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[0].set_ylim([0, 1])
    axs[0].set_xlabel("iteration")
    axs[0].set_ylabel("precision")
    axs[0].xaxis.label.set_color('dimgrey')
    axs[0].yaxis.label.set_color('dimgrey')
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    # axs[0].set_title('Precision per iteration')

    ### Draw the recall graph
    axs[1].axhline(y=stats["context"]["recall_threshold"], color='r', linestyle='--')
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["recall"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[1].plot(it_ax,
                stats['classifier'][other_class]["recall"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[1].set_ylim([0, 1])
    axs[1].set_xlabel("iteration")
    axs[1].set_ylabel("recall")
    axs[1].xaxis.label.set_color('dimgrey')
    axs[1].yaxis.label.set_color('dimgrey')
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    # axs[1].set_title('Recall per iteration')

    ### Draw the Area under PRC graph
    axs[2].plot(it_ax,
                stats['classifier'][target_class]["prc"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["prc"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('Area under PRC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'precision_recall_graph.png'))

    # 3rd Graph: F1-Score, Area under ROC, and MCC score
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the F1-Score graph
    axs[0].plot(it_ax,
                stats['classifier'][target_class]["f_measure"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[0].plot(it_ax,
                stats['classifier'][other_class]["f_measure"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('F1-Score per iteration')

    ### Draw the recall graph
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["roc"],
                ls='-',
                marker='x',
                color='blue')
    axs[1].plot(it_ax,
                stats['classifier'][other_class]["roc"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('Area under ROC per iteration')

    ### Draw the MCC Score
    axs[2].plot(it_ax,
                stats['classifier'][target_class]["mcc"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["mcc"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('MCC score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'f1_roc_mcc_graph.png'))

    # 4th graph: ratio of class and other class per iteration
    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.plot(it_ax,
            stats['datasets']["target_class_ratio"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.plot(it_ax,
            stats['datasets']["other_class_ratio"],
            ls='-',
            marker='^',
            color='orange',
            label=other_class)
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Ratio of classes per iteration")

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'class_ratio_graph.png'))
# End def generate_graphics


# ===== ( Main Loop ) ==================================================================================================

if __name__ == '__main__':

    # Create the folder structure and fetch the output files
    _create_folder_structure()
    _fetch_output_files()

    file_list = os.listdir(datasets_dir)

    # Get the statistics
    with open(join(res_dir, "stats.json"), 'r') as jl:
        stats = json.load(jl)

    # Extract some more statistics from the datasets
    print("Extracting information from the datasets")
    raw_files = [join(datasets_dir, f) for f in listdir(datasets_dir) if isfile(join(datasets_dir, f)) and f.endswith("raw.csv")]
    csv.merge(csv_in=raw_files,
              csv_out=join(datasets_dir, "merged_raw.csv"),
              in_sep=[';'] * len(raw_files),
              out_sep=';')
    df = pd.read_csv(join(datasets_dir, "merged_raw.csv"), sep=";")

    target_class = stats["context"]["target_class"]
    other_class  = stats["context"]["other_class"]
    stats["datasets"] = dict()
    stats["datasets"]["nb_of_instances"]        = df.shape[0]
    stats["datasets"]["nb_of_traces"]           = df["error_trace"].count()
    stats["datasets"]["nb_of_unknown_err"]      = (df.error_reason == 'Unknown').sum()
    stats["datasets"]["nb_of_unknown_eff"]      = (df.error_effect == 'Unknown').sum()
    stats["datasets"]["nb_of_error_type"]       = (df.error_type == 'Unknown').sum()
    stats["datasets"]["nb_switch_disc"]         = (df.error_effect == 'switch_disconnected').sum()
    stats["datasets"]["target_class_ratio"]     = list()
    stats["datasets"]["other_class_ratio"]      = list()

    # Extract the ratio of each class
    for i in range(len([x for x in file_list if 'raw' not in x])):
        # Read the csv file corresponding to the iteration
        df = pd.read_csv(join(datasets_dir, "it_{}.csv".format(i)), sep=";")
        # Get the class count
        classes_count = df['class'].value_counts()
        total = classes_count[target_class] + classes_count[other_class]
        stats["datasets"]["target_class_ratio"] += [classes_count[target_class] / total]
        stats["datasets"]["other_class_ratio"]  += [classes_count[other_class] / total]

    # Create the report
    print("Generating the report")
    _generate_report(stats)

    # Create the graph
    print("Generating the graphs")
    _generate_graphics(stats)

    print("Done")

# End main
