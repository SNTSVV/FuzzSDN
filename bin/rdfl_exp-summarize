#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import datetime
import json
import os
import shutil
from concurrent.futures import ThreadPoolExecutor
from os import listdir
from os.path import isfile, join

import numpy as np
import pandas as pd
import paramiko
import seaborn as sn
from matplotlib import pyplot as plt

from rdfl_exp.utils import csv

nodes = {
    'ORVM00': {
        'enable'        : True,
        'IP'            : "10.240.5.104",
        'port'          : 22,
        'username'      : 'ubuntu',
        'password'      : 'ubuntu',
        'exp_to_fetch'  : "20220118_100227"
    },
    'ORVM01': {
        'enable'        : False,
        'IP'            : "10.240.5.110",
        'port'          : 22,
        'username'      : 'ubuntu',
        'password'      : 'ubuntu',
        'exp_to_fetch'  : "20220118_100300"
    },
    'ORVM02': {
        'enable'        : False,
        'IP'            : "10.240.5.111",
        'port'          : 22,
        'username'      : 'ubuntu',
        'password'      : 'ubuntu',
        'exp_to_fetch'  : "20220118_100227"
    },
    'ORVM03': {
        'enable'        : False,
        'IP'            : "10.240.5.118",
        'port'          : 22,
        'username'      : 'ubuntu',
        'password'      : 'ubuntu',
        'exp_to_fetch'  : "20220118_100300"
    },
    'ORVM04': {
        'enable'        : False,
        'IP'            : "10.240.5.119",
        'port'          : 22,
        'username'      : 'ubuntu',
        'password'      : 'ubuntu',
        'exp_to_fetch'  : "20220118_100300"
    },
}

# host                = "10.240.5.104"  # ORVM00
host                = "10.240.5.110"  # ORMV01
# host                = "10.240.5.111"  # ORVM02
# host                = "10.240.5.118"  # ORVM03
# host                = "10.240.5.119"  # ORVM04
port                = 22
username            = "ubuntu"
password            = "ubuntu"

# exp_to_fetch        = "20220314_181444"  # ORVM00 - No SMOTE
exp_to_fetch        = "20220316_162822"  # ORVM01 - SMOTE threshold of 0.15, target ratio of 0.49
# exp_to_fetch        = "20220316_163810"  # ORVM02   - SMOTE target ratio of 0.49, mutation-rate of 0.6
# exp_to_fetch        = "20220318_142941"  # ORVM03 - SMOTE target ratio of 0.49, mutation-rate of 0.3
# exp_to_fetch        = "20220315_175030"  # ORVM04 - used for tests

print_evl_data      = False
plot_other_class    = False
display_graphs      = False

report_dir          = os.path.expanduser("~/.rdfl_exp/report")
res_dir             = os.path.join(report_dir,
                                   "exp-{}_from-{}_at-{}".format(exp_to_fetch,
                                                                 host,
                                                                 datetime.datetime.now().strftime('%Y%m%d_%H%M%S')))
datasets_dir        = join(res_dir, "datasets")
graph_dir           = join(res_dir, "graphics")
cm_dir              = join(res_dir, "confusion_matrices")
remote_out_dir      = "/home/{}/.local/share/rdfl_exp/out".format(username)


# ===== ( Helper functions ) ===========================================================================================

def _create_folder_structure():
    # Create root and report folder
    dirs = ["{}/.rdfl_exp/".format(os.path.expanduser("~")), report_dir]

    for d in dirs:
        try:
            os.mkdir(d)
        except OSError:
            pass

    if os.path.exists(res_dir):
        shutil.rmtree(res_dir)
    os.mkdir(res_dir)
    os.mkdir(datasets_dir)
    os.mkdir(graph_dir)
    os.mkdir(cm_dir)
# End def create_folder_structure


def _fetch_output_files():
    # Create the ssh connection
    print("Establishing SSH connection to {}@{}:{}".format(username, host, port))
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(host, port, username, password)

    # List all the experiments that can be listed
    stdin, stdout, stderr = ssh.exec_command("ls {}".format(remote_out_dir))
    experiments = sorted([x.strip() for x in stdout.readlines()])

    # Get the experiment we want to fetch
    if exp_to_fetch not in experiments:
        print("\"{}\" not in experiments. Available ones are:{}".format(exp_to_fetch, "\n\t- ".join(experiments)))
        raise SystemExit(0)
    last_exp = experiments[experiments.index(exp_to_fetch)]

    # Get the file from the experiment we want to fetch
    transport = paramiko.Transport((host, port))
    transport.connect(username=username, password=password)
    sftp = paramiko.SFTPClient.from_transport(transport)
    sftp.get(join(remote_out_dir, last_exp, "stats.json"), join(res_dir, "stats.json"))
    file_list = sftp.listdir(join(remote_out_dir, last_exp, "datasets"))
    transport.close()
    sftp.close()

    def download(item):
        # print("Downloading item: {}".format(item))
        transport = None
        sftp = None
        try:
            transport = paramiko.Transport((host, port))
            transport.connect(username=username, password=password)
            sftp = paramiko.SFTPClient.from_transport(transport)
            sftp.get(join(remote_out_dir, last_exp, "datasets", item), join(datasets_dir, item))
        finally:
            if transport:
                transport.close()
            if sftp:
                sftp.close()

    # Lunch the downloads
    with ThreadPoolExecutor(max_workers=10) as pool:
        pool.map(download, file_list)
# End def fetch_output_files

# ===== (  Helper functions ) ================================================================================


def _generate_report(stats: dict):
    #
    rules = stats['learning']['results']['rules'] 
    target_class = stats['context']['target_class']
    
    # Write about the context
    if len(stats['context']['criterion']['kwargs']) > 0:
        criterion_kwargs = ", ".join('{}: {}'.format(key, stats['context']['criterion']['kwargs'][key]) for key in stats['context']['criterion']['kwargs'].keys())
        criterion_kwargs = "({})".format(criterion_kwargs)
    else:
        criterion_kwargs = ''
    
    lines = [
        "======== Context ========\n\n",
        "Scenario: {}\n".format(stats['context']['scenario']),
        "Criterion: {} {}\n".format(stats['context']['criterion']['name'], criterion_kwargs),
        "Target class: \"{}\" (other class: \"{}\")\n".format(stats['context']['target_class'], stats['context']['other_class']),
        "Samples per iteration: {}\n".format(stats['context']['samples_per_iteration']),
        "Mutation enabled: {}\n".format(stats['context']['enable_mutation']),
        "Mutation rate: {}\n".format(stats['context']['mutation_rate'] if stats['context']['enable_mutation'] is True else "N/A"),
        # "Data formatting method used: {}\n\n".format(stats['context']['data_format_method']),
        "\n======== Iteration Summary ========\n\n"
    ]

    for i in range(stats["context"]["iterations"]):
        # First compute the rule string
        rule_str = ""
        nb_of_rules = 0
        for rule in rules[i]:
            nb_of_rules += 1
            rule_str += "\t\tID: {} | CON: {:.5f} | REL_CON: {:.5f} | BGT: {:.5f} | REPR: {}\n".format(
                rule['id'],
                rule['confidence'],
                rule['relative_confidence'],
                rule['budget'],
                rule['repr']
            )

        # Iteration header
        lines.append("=> iteration {}:\n\n".format(i))

        # Write the precision and recall stats achieved
        if 'pp_filter' in stats['learning']['context'][i]:
            lines.append("\tBalancing: {}\n".format(stats['learning']['context'][i]["pp_filter"]["strategy"]))
            lines.append("\t\tStrategy: {}\n".format(stats['learning']['context'][i]["pp_filter"]["strategy"]))
            lines.append("\t\tNeighbors: {}\n".format(stats['learning']['context'][i]["pp_filter"]["neighbors"]))
            lines.append("\t\tPercentage: {}\n".format(stats['learning']['context'][i]["pp_filter"]["factor"]))
        lines.append("\tcross-val metrics:\n")
        lines.append("\t\tPrecision for target class: {}\n".format(stats['learning']['results']['cross-validation'][target_class]["precision"][i]))
        lines.append("\t\tRecall for target class: {}\n\n".format(stats['learning']['results']['cross-validation'][target_class]["recall"][i]))

        # Write the time used per iteration:
        iteration_time = str(datetime.timedelta(seconds=float(stats["timing"]["iteration"][i])))
        learning_time = str(datetime.timedelta(seconds=float(stats["timing"]["learning"][i])))
        lines.append("\tTime taken to finish the iteration: {}\n".format(iteration_time))
        lines.append("\ttime taken to generate the classifier: {}\n\n".format(learning_time))

        # Add the rules
        lines.append("\tNumber of Rules generated: {}\n".format(nb_of_rules))
        if nb_of_rules > 0:
            lines.append("\tRules:\n")
            lines.append(rule_str)
        lines.append("\n")

    # Write the lines to the report file
    with open(os.path.join(res_dir, "report.txt"), 'w') as f:
        f.writelines(lines)
# End def _generate_report


def _generate_graphics(stats: dict, show: bool = False):
    """Generate the graphics for the report
    :param stats: The statistics dictionary
    :param show: If set to True, the graphs are display using plt
    """

    it_ax = list(range(stats["context"]["iterations"]))
    rules = stats['learning']['results']['rules']
    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    # update the font size
    plt.rcParams.update({'font.size': 22})

    # ===== 1st Graph: Number of rules per iteration ===================================================================

    nb_of_rules = [len(rules[i]) for i in range(stats["context"]["iterations"])]
    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the number of rules graph
    ax.plot(it_ax,
            nb_of_rules,
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ### Draw the regression line for the number of rules generated
    x = np.array(range(stats["context"]["iterations"]))
    y = np.array(nb_of_rules)
    coef = np.polyfit(x, y, 1)
    ax.plot(x,
            coef[0] * x + coef[1],
            ls='--',
            color='orange')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Number of rules per iteration")
    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'nb_rules_graph.png'))

    # ===== 2nd Graph: Accuracy ========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the precision graph
    ax.plot(it_ax,
            stats['learning']['results']['accuracy'],
            ls='-',
            marker='x',
            color='blue',
            label="accuracy_score")
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Classifier Accuracy per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'accuracy_graph.png'))

    # ===== 3rd Graph TP/FP rate for target_class, TP/FP rate for other class ==========================================

    fig, axs = plt.subplots(1, 2, figsize=(21, 7), dpi=96)
    axs[0].plot(it_ax,
                stats['learning']['results']['cross-validation'][target_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[0].plot(it_ax,
                stats['learning']['results']['cross-validation'][target_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('TP/FP Rate for {} per iteration'.format(target_class))

    ### Draw the TP/FP graph for the other class
    axs[1].plot(it_ax,
                stats['learning']['results']['cross-validation'][other_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[1].plot(it_ax,
                stats['learning']['results']['cross-validation'][other_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('TP/FP Rate for {}'.format(other_class))

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'tp_fp_rate_graph.png'))

    # ===== 4th graph: Precision =======================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the precision graph
    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]["precision"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    if print_evl_data:
        ax.plot(it_ax,
                stats['learning']['results']['evaluation'][target_class]["precision"],
                ls='--',
                marker='x',
                color='blue',
                label='{}-evl'.format(target_class))
    if plot_other_class:
        ax.plot(it_ax,
                stats['learning']['results']['cross-validation'][other_class]["precision"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("precision")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Precision per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'precision_graph.png'))

    # ===== 5th graph: Recall ==========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]["recall"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    if print_evl_data:
        ax.plot(it_ax,
                stats['learning']['results']['evaluation'][target_class]['recall'],
                ls='--',
                marker='x',
                color='blue',
                label='{}-evl'.format(target_class))
    if plot_other_class:
        ax.plot(it_ax,
                stats['learning']['results']['evaluation'][other_class]['recall'],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("recall")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Recall per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'recall_graph.png'))

    # ===== 6th graph: F1-Score ========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]['f_measure'],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    if plot_other_class:
        ax.plot(it_ax,
                stats['learning']['results']['cross-validation'][other_class]['f_measure'],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('F1-Score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'f1_graph.png'))

    # ===== 7th graph: Area under ROC ==================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)

    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]["roc"],
            ls='-',
            marker='x',
            color='blue')
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Area under ROC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'auc_roc_graph.png'))

    # ===== 8th graph: Area under PRC ==================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]["prc"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("prc in %")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Area under PRC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'auc_prc_graph.png'))

    # ===== 9th graph: MCC =============================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    min_mcc = min(0 if v is None else v for v in stats['learning']['results']['cross-validation'][target_class]["mcc"])
    max_mcc = max(0 if v is None else v for v in stats['learning']['results']['cross-validation'][target_class]["mcc"])
    min_ax = 0 if min_mcc > 0 else min_mcc - 0.25
    max_ax = 1 if max_mcc > 0.5 else max_mcc + 0.25
    ax.plot(it_ax,
            stats['learning']['results']['cross-validation'][target_class]["mcc"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.set_ylim([min_ax, max_ax])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('MCC score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'mcc_graph.png'))

    # ===== 10th graph: ratio of class and other class instances =======================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.set_ylim([0, 1])
    ax.axhline(y=0.5, color='grey', linestyle='--')
    ax.plot(it_ax,
            stats['learning']['data']['class_ratio'],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Class Ratio per iteration")

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'class_ratio_graph.png'))

    # ===== 11th graph: Number of instances evolution ==================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    total_count = []
    # BUG: stats['learning']['data']['total_count'] returns a list of with target count and other count instead of
    #      the total
    for i in range(stats['context']['iterations']):
        total_count.append(stats['learning']['data']['target_count'][i] + stats['learning']['data']['other_count'][i])

    ax.set_ylim([0, max(total_count)])
    ax.plot(it_ax,
            stats['learning']['data']['target_count'],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.plot(it_ax,
            stats['learning']['data']['other_count'],
            ls='-',
            marker='^',
            color='orange',
            label=other_class)
    ax.plot(it_ax,
            total_count,
            ls='--',
            marker='o',
            color='dimgrey',
            label='total_count')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="upper left")
    ax.set_title("Number of instances per iteration")

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'number_instances_per_class_graph.png'))
# End def _generate_graphics


def _generate_confusion_matrices(stats: dict):
    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    for i in range(stats["context"]["iterations"]):

        plt.clf()

        # Generate the heat map arrays

        tar_tp = stats['learning']['results']['cross-validation'][target_class]["tp_rate"][i]
        oth_fp = stats['learning']['results']['cross-validation'][other_class]["fp_rate"][i]
        tgt_fp = stats['learning']['results']['cross-validation'][target_class]["fp_rate"][i]
        oth_tp = stats['learning']['results']['cross-validation'][other_class]["tp_rate"][i]
        tar_tp = 0 if tar_tp is None else tar_tp
        oth_fp = 0 if oth_fp is None else oth_fp
        tgt_fp = 0 if tgt_fp is None else tgt_fp
        oth_tp = 0 if oth_tp is None else oth_tp

        array = [[tar_tp, oth_fp],
                 [tgt_fp, oth_tp]]

        df_cm = pd.DataFrame(array, range(2), range(2))

        # Generate the heatmap
        sn.set(font_scale=1.4)  # for label size
        svm = sn.heatmap(
            df_cm,
            annot=True,
            annot_kws={"size": 16},
            xticklabels=[target_class, other_class],
            yticklabels=[target_class, other_class]
        )

        plt.title("confusion matrix iteration {}".format(i))
        plt.xlabel("predicted")
        plt.ylabel("actual")

        # Save the figure
        fig = svm.get_figure()
        fig.savefig(os.path.join(cm_dir, 'cm_it_{}.png'.format(i)))
# End def _generate_confusion_matrices


# ===== ( Main Loop ) ==================================================================================================

if __name__ == '__main__':

    print("Fetching files from {}@{}:{} for experiment {}...".format(username, host, port, exp_to_fetch))
    # Create the folder structure and fetch the output files
    _create_folder_structure()
    _fetch_output_files()

    file_list = os.listdir(datasets_dir)

    # Get the statistics
    with open(join(res_dir, "stats.json"), 'r') as jl:
        stats = json.load(jl)

    # Extract some more statistics from the datasets
    print("Extracting information from the datasets")
    raw_files = [join(datasets_dir, f) for f in listdir(datasets_dir) if
                 isfile(join(datasets_dir, f)) and f.endswith("raw.csv")]
    csv.merge(csv_in=raw_files,
              csv_out=join(datasets_dir, "merged_raw.csv"),
              in_sep=[';'] * len(raw_files),
              out_sep=';')
    df = pd.read_csv(join(datasets_dir, "merged_raw.csv"), sep=";")

    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    # Create the report
    print("Generating the report")
    _generate_report(stats)

    # Create the graph
    print("Generating the graphs")
    _generate_graphics(stats, display_graphs)

    # Create the confusion matrices
    _generate_confusion_matrices(stats)
    print("Done")
# End main
