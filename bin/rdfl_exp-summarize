#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import datetime
import json
import os
import shutil
from os import listdir
from os.path import isfile, join

import numpy as np
import pandas as pd
import paramiko
import seaborn as sn
from matplotlib import pyplot as plt

from rdfl_exp import config
from rdfl_exp.utils import csv
from rdfl_exp.utils.terminal import progress_bar

host                = "10.240.5.104"
port                = 22
username            = "ubuntu"
password            = "ubuntu"

exp_to_fetch        = "20211207_181723"
old_data            = False
print_evl_data      = False
plot_other_class    = False
display_graphs      = False

report_dir          = os.path.join(config.RDFL_ROOT, "report")
res_dir             = os.path.join(report_dir, "rdfl_exp_summary_{}".format(exp_to_fetch))
datasets_dir        = join(res_dir, "datasets")
graph_dir           = join(res_dir, "graphics")
cm_dir              = join(res_dir, "confusion_matrices")
remote_out_dir      = "/home/ubuntu/.rdfl_exp/out"


# ===== ( Helper functions ) ===========================================================================================

def _create_folder_structure():
    # Create root and report folder
    dirs = [config.RDFL_ROOT, report_dir]

    for d in dirs:
        try:
            os.mkdir(d)
        except OSError:
            pass

    if os.path.exists(res_dir):
        shutil.rmtree(res_dir)
    os.mkdir(res_dir)
    os.mkdir(datasets_dir)
    os.mkdir(graph_dir)
    os.mkdir(cm_dir)
# End def create_folder_structure


def _fetch_output_files():
    # Create the ssh connection
    print("Establishing SSH connection to {}@{}:{}".format(username, host, port))
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(host, port, username, password)

    # List all the experiments that can be listed
    stdin, stdout, stderr = ssh.exec_command("ls {}".format(remote_out_dir))
    experiments = sorted([x.strip() for x in stdout.readlines()])

    # Get the experiment we want to fetch
    last_exp = experiments[experiments.index(exp_to_fetch)]

    # Get the file from the experiment we want to fetch
    transport = paramiko.Transport((host, port))
    sftp = None
    try:
        print("Establishing the transport pipe")
        transport.connect(username=username, password=password)
        sftp = paramiko.SFTPClient.from_transport(transport)

        print("Getting the statistics file")
        sftp.get(join(remote_out_dir, last_exp, "stats.json"), join(res_dir, "stats.json"))
        # Get the datasets

        print("Getting the datasets")
        file_list = sftp.listdir(join(remote_out_dir, last_exp, "datasets"))
        it = 1
        progress_bar(0, len(file_list),
                     prefix='Progress:',
                     suffix='Complete ({}/{})'.format(0, len(file_list)),
                     length=100)
        for item in file_list:
            sftp.get(join(remote_out_dir, last_exp, "datasets", item), join(datasets_dir, item))
            progress_bar(it, len(file_list),
                         prefix='Progress:',
                         suffix='Complete ({}/{})'.format(it, len(file_list)),
                         length=100)
            it += 1

    finally:
        if transport:
            transport.close()
        if sftp:
            sftp.close()
# End def fetch_output_files


def _generate_report(stats: dict):
    #
    rules = stats["classifier"]["rules"]
    target_class = stats['context']['target_class']

    # Write about the context
    lines = [
        "======== Context ========\n\n",
        # "Precision Threshold: {:.2f}%\n".format(stats['context']['precision_threshold'] * 100.0),
        # "Recall Threshold: {:.2f}%\n".format(stats['context']['recall_threshold'] * 100.0),
        "target_class: {}\n".format(stats['context']['target_class']),
        "Samples generated per iterations: {}\n".format(stats['context']['samples_per_iteration']),
        "mutation of additional fields: {}\n".format(stats['context']['enable_mutation']),
        "mutation rate: {}\n".format(stats['context']['mutation_rate']) if stats['context']['enable_mutation'] is True else "",
        # "Data formatting method used: {}\n\n".format(stats['context']['data_format_method']),
        "======== Iteration Summary ========\n\n"
    ]
    for i in range(stats["context"]["iterations"]):
        # First compute the rule string
        rule_str = ""
        nb_of_rules = 0
        for r in rules[i]:
            nb_of_rules += 1
            rule_str += "\t\t{}\n".format(r)

        # Iteration header
        lines.append("=> iteration :{}\n\n".format(i))

        # Write the precision and recall stats achieved
        if 'ml_ctx' in stats.keys():
            lines.append("\tBalancing: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["strategy"]))
            lines.append("\t\tStrategy: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["strategy"]))
            lines.append("\t\tNeighbors: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["neighbors"]))
            lines.append("\t\tFactor: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["factor"]))
        lines.append("\tcross-val metrics:\n")
        lines.append("\t\tPrecision for target class: {}\n".format(
            stats["classifier"]['cross-validation'][target_class]["precision"][i]))
        lines.append("\t\tRecall for target class: {}\n\n".format(
            stats["classifier"]['cross-validation'][target_class]["recall"][i]))

        # Write the time used per iteration:
        iteration_time = str(datetime.timedelta(seconds=float(stats["timing"]["iteration"][i])))
        learning_time = str(datetime.timedelta(seconds=float(stats["timing"]["learning"][i])))
        lines.append("\tTime taken to finish the iteration: {}\n".format(iteration_time))
        lines.append("\ttime taken to generate the classifier: {}\n\n".format(learning_time))

        # Add the rules
        lines.append("\tNumber of Rules generated: {}\n".format(nb_of_rules))
        if nb_of_rules > 0:
            lines.append("\tRules:\n")
            lines.append(rule_str)
        lines.append("\n")

    # Write the lines to the report file
    with open(os.path.join(res_dir, "report.txt"), 'w') as f:
        f.writelines(lines)
# End def _generate_report


def _generate_graphics(stats: dict, show: bool = False):
    """Generate the graphics for the report
    :param stats: The statistics dictionnary
    :param show: If set to True, the graphs are display using plt
    """

    it_ax = list(range(stats["context"]["iterations"]))
    rules = stats["classifier"]["rules"]
    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    # update the font size
    plt.rcParams.update({'font.size': 22})

    # ===== 1st Graph: Number of rules per iteration ===================================================================

    nb_of_rules = [len(rules[i]) for i in range(stats["context"]["iterations"])]
    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the number of rules graph
    ax.plot(it_ax,
            nb_of_rules,
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ### Draw the regression line for the number of rules generated
    x = np.array(range(stats["context"]["iterations"]))
    y = np.array(nb_of_rules)
    coef = np.polyfit(x, y, 1)
    ax.plot(x,
            coef[0] * x + coef[1],
            ls='--',
            color='orange')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Number of rules per iteration")
    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'nb_rules_graph.png'))

    # ===== 2nd Graph: Accuracy ========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the precision graph
    ax.plot(it_ax,
            stats['classifier']["accuracy"],
            ls='-',
            marker='x',
            color='blue',
            label="accuracy_score")
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Classifier Accuracy per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'accuracy_graph.png'))

    # ===== 3rd Graph TP/FP rate for target_class, TP/FP rate for other class ==========================================

    fig, axs = plt.subplots(1, 2, figsize=(21, 7), dpi=96)
    axs[0].plot(it_ax,
                stats['classifier']['cross-validation'][target_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[0].plot(it_ax,
                stats['classifier']['cross-validation'][target_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('TP/FP Rate for {} per iteration'.format(target_class))

    ### Draw the TP/FP graph for the other class
    axs[1].plot(it_ax,
                stats['classifier']['cross-validation'][other_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[1].plot(it_ax,
                stats['classifier']['cross-validation'][other_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('TP/FP Rate for {}'.format(other_class))

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'tp_fp_rate_graph.png'))

    # ===== 4th graph: Precision =======================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the precision graph
    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["precision"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    if print_evl_data:
        ax.plot(it_ax,
                stats['classifier']['evaluation'][target_class]["precision"],
                ls='--',
                marker='x',
                color='blue',
                label='{}-evl'.format(target_class))
    if plot_other_class:
        ax.plot(it_ax,
                stats['classifier']['cross-validation'][other_class]["precision"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("precision")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Precision per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'precision_graph.png'))

    # ===== 5th graph: Recall ==========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["recall"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    if print_evl_data:
        ax.plot(it_ax,
                stats['classifier']['evaluation'][target_class]["recall"],
                ls='--',
                marker='x',
                color='blue',
                label='{}-evl'.format(target_class))
    if plot_other_class:
        ax.plot(it_ax,
                stats['classifier'][other_class]["recall"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("recall")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Recall per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'recall_graph.png'))

    # ===== 6th graph: F1-Score ========================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["f_measure"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    if plot_other_class:
        ax.plot(it_ax,
                stats['classifier'][other_class]["f_measure"],
                ls='-',
                marker='^',
                color='orange',
                label=other_class)
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('F1-Score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'f1_graph.png'))

    # ===== 7th graph: Area under ROC ==================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)

    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["roc"],
            ls='-',
            marker='x',
            color='blue')
    ax.set_ylim([0, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Area under ROC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'auc_roc_graph.png'))

    # ===== 8th graph: Area under PRC ==================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["prc"],
            ls='-',
            marker='x',
            color='blue',
            label='{}-cv'.format(target_class) if print_evl_data else target_class)
    ax.set_ylim([0, 1])
    ax.set_xlabel("iteration")
    ax.set_ylabel("prc in %")
    ax.xaxis.label.set_color('dimgrey')
    ax.yaxis.label.set_color('dimgrey')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('Area under PRC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'auc_prc_graph.png'))

    # ===== 9th graph: MCC =============================================================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ax.plot(it_ax,
            stats['classifier']['cross-validation'][target_class]["mcc"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.set_ylim([-1, 1])
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title('MCC score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'mcc_graph.png'))

    # ===== 10th graph: ratio of class and other class instances =======================================================

    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.set_ylim([0, 1])
    ax.axhline(y=0.5, color='grey', linestyle='--')
    ax.plot(it_ax,
            stats['datasets']["target_class_ratio"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.plot(it_ax,
            stats['datasets']["other_class_ratio"],
            ls='-',
            marker='^',
            color='orange',
            label=other_class)
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Ratio of classes per iteration")

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(graph_dir, 'class_ratio_graph.png'))
# End def _generate_graphics


def _generate_confusion_matrices(stats: dict):
    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    for i in range(stats["context"]["iterations"]):

        plt.clf()

        # Generate the heat map arrays
        array = [[stats['classifier']['cross-validation'][target_class]["tp_rate"][i],
                  stats['classifier']['cross-validation'][other_class]["fp_rate"][i]],
                 [stats['classifier']['cross-validation'][target_class]["fp_rate"][i],
                  stats['classifier']['cross-validation'][other_class]["tp_rate"][i]]]

        df_cm = pd.DataFrame(array, range(2), range(2))

        # Generate the heatmap
        sn.set(font_scale=1.4)  # for label size
        svm = sn.heatmap(
            df_cm,
            annot=True,
            annot_kws={"size": 16},
            xticklabels=[target_class, other_class],
            yticklabels=[target_class, other_class]
        )

        plt.title("confusion matrix iteration {}".format(i))
        plt.xlabel("predicted")
        plt.ylabel("actual")

        # Save the figure
        fig = svm.get_figure()
        fig.savefig(os.path.join(cm_dir, 'cm_it_{}.png'.format(i)))
# End def _generate_confusion_matrices

# ===== ( Deprecated Helper functions ) ================================================================================

def _generate_report_deprecated(stats: dict):
    #
    rules = stats["classifier"]["rules"]
    target_class = stats['context']['target_class']

    # Write about the context
    lines = [
        "======== Context ========\n\n",
        # "Precision Threshold: {:.2f}%\n".format(stats['context']['precision_threshold'] * 100.0),
        # "Recall Threshold: {:.2f}%\n".format(stats['context']['recall_threshold'] * 100.0),
        "target_class: {}\n".format(stats['context']['target_class']),
        "Samples generated per iterations: {}\n".format(stats['context']['samples_per_iteration']),
        # "Data formatting method used: {}\n\n".format(stats['context']['data_format_method']),
        "======== Iteration Summary ========\n\n"
    ]
    for i in range(stats["context"]["iterations"]):
        # First compute the rule string
        rule_str = ""
        nb_of_rules = 0
        for r in rules[i]:
            nb_of_rules += 1
            rule_str += "\t\t{}\n".format(r)

        # Iteration header
        lines.append("=> iteration :{}\n\n".format(i))

        # Write the precision and recall stats achieved
        if 'ml_ctx' in stats.keys():
            lines.append("\tBalancing: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["strategy"]))
            lines.append("\t\tStrategy: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["strategy"]))
            lines.append("\t\tNeighbors: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["neighbors"]))
            lines.append("\t\tFactor: {}\n".format(stats["ml_ctx"][i]["pp_filter"]["factor"]))
        lines.append("\tcross-val metrics:\n")
        lines.append("\t\tPrecision for target class: {}\n".format(stats["classifier"][target_class]["precision"][i]))
        lines.append("\t\tRecall for target class: {}\n\n".format(stats["classifier"][target_class]["recall"][i]))

        # Write the time used per iteration:
        iteration_time = str(datetime.timedelta(seconds=float(stats["timing"]["iteration"][i])))
        learning_time = str(datetime.timedelta(seconds=float(stats["timing"]["learning"][i])))
        lines.append("\tTime taken to finish the iteration: {}\n".format(iteration_time))
        lines.append("\ttime taken to generate the classifier: {}\n\n".format(learning_time))

        # Add the rules
        lines.append("\tNumber of Rules generated: {}\n".format(nb_of_rules))
        if nb_of_rules > 0:
            lines.append("\tRules:\n")
            lines.append(rule_str)
        lines.append("\n")

    # Write the lines to the report file
    with open(os.path.join(res_dir, "report.txt"), 'w') as f:
        f.writelines(lines)
# End def generate_report_deprecated


def _generate_graphics_deprecated(stats: dict, show=False):
    """Generate the graphics for the report
    :param stats: The statistics dictionnary
    :param show: If set to True, the graphs are display using plt
    """

    it_ax = list(range(stats["context"]["iterations"]))
    rules = stats["classifier"]["rules"]
    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]

    # update the font size
    plt.rcParams.update({'font.size': 22})

    # 1st Graph: Number of rules per iteration
    nb_of_rules = [len(rules[i]) for i in range(stats["context"]["iterations"])]
    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the number of rules graph
    ax.plot(it_ax,
            nb_of_rules,
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ### Draw the regression line for the number of rules generated
    x = np.array(range(stats["context"]["iterations"]))
    y = np.array(nb_of_rules)
    coef = np.polyfit(x, y, 1)
    ax.plot(x,
            coef[0] * x + coef[1],
            ls='--',
            color='orange')
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Number of rules per iteration")
    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'nb_rules_graph.png'))

    # 2nd Graph: Accuracy, TP/FP rate for target_class, TP/FP rate for other class
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the precision graph
    axs[0].plot(it_ax,
                stats['classifier']["accuracy"],
                ls='-',
                marker='x',
                color='blue',
                label="accuracy_score")
    axs[0].set_ylim([0, 1])
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('Classifier Accuracy per iteration')

    ### Draw the TP/FP graph for the target class
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('TP/FP Rate for {} per iteration'.format(target_class))

    ### Draw the TP/FP graph for the other class
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["tp_rate"],
                ls='-',
                marker='x',
                color='blue',
                label="TP Rate")
    axs[2].plot(it_ax,
                stats['classifier'][other_class]["fp_rate"],
                ls='-',
                marker='x',
                color='red',
                label="FP Rate")
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('TP/FP Rate for {} per iteration'.format(other_class))

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'classifier_accuracy_graph.png'))

    # 3rd Graph: precision, recall, PRC graph
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the precision graph
    # axs[0].axhline(y=stats["context"]["precision_threshold"], color='r', linestyle='--')
    axs[0].plot(it_ax,
                stats['classifier'][target_class]["precision"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    if plot_other_class:
        axs[0].plot(it_ax,
                    stats['classifier'][other_class]["precision"],
                    ls='-',
                    marker='^',
                    color='orange',
                    label=other_class)
    axs[0].set_ylim([0, 1])
    axs[0].set_xlabel("iteration")
    axs[0].set_ylabel("precision")
    axs[0].xaxis.label.set_color('dimgrey')
    axs[0].yaxis.label.set_color('dimgrey')
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('Precision per iteration')

    ### Draw the recall graph
    # axs[1].axhline(y=stats["context"]["recall_threshold"], color='r', linestyle='--')
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["recall"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    if plot_other_class:
        axs[1].plot(it_ax,
                    stats['classifier'][other_class]["recall"],
                    ls='-',
                    marker='^',
                    color='orange',
                    label=other_class)
    axs[1].set_ylim([0, 1])
    axs[1].set_xlabel("iteration")
    axs[1].set_ylabel("recall")
    axs[1].xaxis.label.set_color('dimgrey')
    axs[1].yaxis.label.set_color('dimgrey')
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('Recall per iteration')

    ### Draw the Area under PRC graph
    axs[2].plot(it_ax,
                stats['classifier'][target_class]["prc"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    if plot_other_class:
        axs[2].plot(it_ax,
                    stats['classifier'][other_class]["prc"],
                    ls='-',
                    marker='^',
                    color='orange',
                    label=other_class)
    axs[2].set_ylim([0, 1])
    axs[2].set_xlabel("iteration")
    axs[2].set_ylabel("prc in %")
    axs[2].xaxis.label.set_color('dimgrey')
    axs[2].yaxis.label.set_color('dimgrey')
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('Area under PRC per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'precision_recall_graph.png'))

    # 4th Graph: F1-Score, Area under ROC, and MCC score
    fig, axs = plt.subplots(1, 3, figsize=(32, 7), dpi=96)
    ### Draw the F1-Score graph
    axs[0].plot(it_ax,
                stats['classifier'][target_class]["f_measure"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    if plot_other_class:
        axs[0].plot(it_ax,
                    stats['classifier'][other_class]["f_measure"],
                    ls='-',
                    marker='^',
                    color='orange',
                    label=other_class)
    axs[0].set_ylim([0, 1])
    axs[0].locator_params(axis="x", integer=True, tight=True)
    axs[0].legend(loc="lower right")
    axs[0].set_title('F1-Score per iteration')

    ### Draw the ROC graph
    axs[1].plot(it_ax,
                stats['classifier'][target_class]["roc"],
                ls='-',
                marker='x',
                color='blue')
    axs[1].set_ylim([0, 1])
    axs[1].locator_params(axis="x", integer=True, tight=True)
    axs[1].legend(loc="lower right")
    axs[1].set_title('Area under ROC per iteration')

    ### Draw the MCC Score
    axs[2].plot(it_ax,
                stats['classifier'][target_class]["mcc"],
                ls='-',
                marker='x',
                color='blue',
                label=target_class)
    axs[2].set_ylim([0, 1])
    axs[2].locator_params(axis="x", integer=True, tight=True)
    axs[2].legend(loc="lower right")
    axs[2].set_title('MCC score per iteration')

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'f1_roc_mcc_graph.png'))

    # 5th graph: ratio of class and other class per iteration
    fig, ax = plt.subplots(1, 1, figsize=(10, 7), dpi=96)
    ### Draw the F1-Score graph
    ax.set_ylim([0, 1])
    ax.axhline(y=0.5, color='grey', linestyle='--')
    ax.plot(it_ax,
            stats['datasets']["target_class_ratio"],
            ls='-',
            marker='x',
            color='blue',
            label=target_class)
    ax.plot(it_ax,
            stats['datasets']["other_class_ratio"],
            ls='-',
            marker='^',
            color='orange',
            label=other_class)
    ax.locator_params(axis="x", integer=True, tight=True)
    ax.legend(loc="lower right")
    ax.set_title("Ratio of classes per iteration")

    plt.tight_layout()
    plt.plot()
    if show is True:
        plt.show()
    fig.savefig(os.path.join(res_dir, 'class_ratio_graph.png'))
# End def generate_graphics_deprecated


# ===== ( Main Loop ) ==================================================================================================

if __name__ == '__main__':

    # Create the folder structure and fetch the output files
    _create_folder_structure()
    _fetch_output_files()

    file_list = os.listdir(datasets_dir)

    # Get the statistics
    with open(join(res_dir, "stats.json"), 'r') as jl:
        stats = json.load(jl)

    # Extract some more statistics from the datasets
    print("Extracting information from the datasets")
    raw_files = [join(datasets_dir, f) for f in listdir(datasets_dir) if
                 isfile(join(datasets_dir, f)) and f.endswith("raw.csv")]
    csv.merge(csv_in=raw_files,
              csv_out=join(datasets_dir, "merged_raw.csv"),
              in_sep=[';'] * len(raw_files),
              out_sep=';')
    df = pd.read_csv(join(datasets_dir, "merged_raw.csv"), sep=";")

    target_class = stats["context"]["target_class"]
    other_class = stats["context"]["other_class"]
    stats["datasets"] = dict()
    stats["datasets"]["nb_of_instances"] = df.shape[0]
    stats["datasets"]["nb_of_traces"] = df["error_trace"].count()
    stats["datasets"]["nb_of_unknown_err"] = (df.error_reason == 'Unknown').sum()
    stats["datasets"]["nb_of_unknown_eff"] = (df.error_effect == 'Unknown').sum()
    stats["datasets"]["nb_of_error_type"] = (df.error_type == 'Unknown').sum()
    stats["datasets"]["nb_switch_disc"] = (df.error_effect == 'switch_disconnected').sum()
    stats["datasets"]["target_class_ratio"] = list()
    stats["datasets"]["other_class_ratio"] = list()

    # Extract the ratio of each class
    for i in range(len([x for x in file_list if 'raw' not in x and 'arff' not in x])):
        # Read the csv file corresponding to the iteration
        df = pd.read_csv(join(datasets_dir, "it_{}.csv".format(i)), sep=";")
        # Get the class count
        classes_count = df['class'].value_counts()
        total = classes_count[target_class] + classes_count[other_class]
        stats["datasets"]["target_class_ratio"] += [classes_count[target_class] / total]
        stats["datasets"]["other_class_ratio"] += [classes_count[other_class] / total]

    # Create the report
    print("Generating the report")
    _generate_report(stats) if old_data is False else _generate_report_deprecated(stats)

    # Create the graph
    print("Generating the graphs")
    _generate_graphics(stats, display_graphs) if old_data is False else _generate_graphics_deprecated(stats)

    # Create the confusion matrices
    if old_data is False:
        _generate_confusion_matrices(stats)

    print("Done")

# End main
